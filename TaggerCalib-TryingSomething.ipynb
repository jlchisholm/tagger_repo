{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used for testing things with calibrating the tagger timing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (Possible) Code for Calibrating the Tagger Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the tools we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.16/00\n"
     ]
    }
   ],
   "source": [
    "import Acqu as aq\n",
    "import AcquDetector as aqdet\n",
    "import ROOT\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from rootpy.plotting import histogram, Hist2D, Hist, Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data to be analyzed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mk2 Data\n"
     ]
    }
   ],
   "source": [
    "inFile = '/scratch/2019-05_Timepix/Timepix_33.dat'\n",
    "aq.openFile(inFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the detector file for the Tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taggerNewer.json\n"
     ]
    }
   ],
   "source": [
    "aqdet.LoadDetectors(['taggerNewer.json'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up histograms for each of the channels, and then fill them with tagger times from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Events Processed: ', 5000)\n",
      "('Events Processed: ', 10000)\n",
      "('Events Processed: ', 15000)\n",
      "('Events Processed: ', 20000)\n",
      "('Events Processed: ', 25000)\n",
      "('Events Processed: ', 30000)\n",
      "('Events Processed: ', 35000)\n",
      "('Events Processed: ', 40000)\n",
      "('Events Processed: ', 45000)\n",
      "('Events Processed: ', 50000)\n",
      "('Events Processed: ', 55000)\n",
      "('Events Processed: ', 60000)\n",
      "('Events Processed: ', 65000)\n",
      "('Events Processed: ', 70000)\n",
      "('Events Processed: ', 75000)\n",
      "('Events Processed: ', 80000)\n",
      "('Events Processed: ', 85000)\n",
      "('Events Processed: ', 90000)\n",
      "('Events Processed: ', 95000)\n",
      "('Events Processed: ', 100000)\n",
      "('Events Processed: ', 105000)\n",
      "('Events Processed: ', 110000)\n",
      "('Events Processed: ', 115000)\n",
      "('Events Processed: ', 120000)\n",
      "('Events Processed: ', 125000)\n",
      "('Events Processed: ', 130000)\n",
      "('Events Processed: ', 135000)\n",
      "('Events Processed: ', 140000)\n",
      "('Events Processed: ', 145000)\n",
      "('Events Processed: ', 150000)\n",
      "('Events Processed: ', 155000)\n",
      "('Events Processed: ', 160000)\n",
      "('Events Processed: ', 165000)\n",
      "('Events Processed: ', 170000)\n",
      "('Events Processed: ', 175000)\n",
      "('Events Processed: ', 180000)\n",
      "('Events Processed: ', 185000)\n",
      "('Events Processed: ', 190000)\n",
      "('Events Processed: ', 195000)\n",
      "('Events Processed: ', 200000)\n",
      "('Events Processed: ', 205000)\n",
      "('Events Processed: ', 210000)\n",
      "('Events Processed: ', 215000)\n",
      "('Events Processed: ', 220000)\n",
      "('Events Processed: ', 225000)\n",
      "('Events Processed: ', 230000)\n",
      "('Events Processed: ', 235000)\n",
      "('Events Processed: ', 240000)\n",
      "('Events Processed: ', 245000)\n",
      "('Events Processed: ', 250000)\n",
      "('Events Processed: ', 255000)\n",
      "('Events Processed: ', 260000)\n",
      "('Events Processed: ', 265000)\n",
      "('Events Processed: ', 270000)\n",
      "('Events Processed: ', 275000)\n",
      "('Events Processed: ', 280000)\n",
      "('Events Processed: ', 285000)\n",
      "('Events Processed: ', 290000)\n",
      "('Events Processed: ', 295000)\n",
      "('Events Processed: ', 300000)\n",
      "('Events Processed: ', 305000)\n",
      "('Events Processed: ', 310000)\n",
      "('Events Processed: ', 315000)\n",
      "('Events Processed: ', 320000)\n",
      "('Events Processed: ', 325000)\n",
      "('Events Processed: ', 330000)\n",
      "('Events Processed: ', 335000)\n",
      "('Events Processed: ', 340000)\n",
      "('Events Processed: ', 345000)\n",
      "('Events Processed: ', 350000)\n",
      "('Events Processed: ', 355000)\n",
      "('Events Processed: ', 360000)\n",
      "('Events Processed: ', 365000)\n",
      "('Events Processed: ', 370000)\n",
      "('Events Processed: ', 375000)\n",
      "('Events Processed: ', 380000)\n",
      "('Events Processed: ', 385000)\n",
      "('Events Processed: ', 390000)\n",
      "('Events Processed: ', 395000)\n",
      "('Events Processed: ', 400000)\n",
      "('Events Processed: ', 405000)\n",
      "('Events Processed: ', 410000)\n",
      "('Events Processed: ', 415000)\n",
      "('Events Processed: ', 420000)\n",
      "('Events Processed: ', 425000)\n",
      "('Events Processed: ', 430000)\n",
      "('Events Processed: ', 435000)\n",
      "('Events Processed: ', 440000)\n",
      "('Events Processed: ', 445000)\n",
      "('Events Processed: ', 450000)\n",
      "('Events Processed: ', 455000)\n",
      "('Events Processed: ', 460000)\n",
      "('Events Processed: ', 465000)\n",
      "('Events Processed: ', 470000)\n",
      "('Events Processed: ', 475000)\n",
      "('Events Processed: ', 480000)\n",
      "('Events Processed: ', 485000)\n",
      "('Events Processed: ', 490000)\n",
      "('Events Processed: ', 495000)\n",
      "('Events Processed: ', 500000)\n"
     ]
    }
   ],
   "source": [
    "taggerChannels = aqdet.Channels['Tagger']     # number of channels in the tagger (368)\n",
    "\n",
    "histos = [None]*taggerChannels                # set up an array of 368 histograms to fill\n",
    "energies = [None]*taggerChannels\n",
    "\n",
    "for i in range(taggerChannels):               # for each tagger channel\n",
    "    histos[i] = Hist(1300,-500,800)           # set up a histogram for that tagger channel\n",
    "    energies[i] = Hist(1000,0,1000)\n",
    "\n",
    "def plotCalTagger():\n",
    "    data = aqdet.Calibrate(aq.adcArray)              # calibrate the data\n",
    "    taggerTimes = aqdet.Arrays['Tagger']['Time']     # get the tagger times\n",
    "    taggerEnergies = aqdet.Arrays['Tagger']['Time']['Energy']\n",
    "    \n",
    "    for dat in taggerTimes:                          # for each data point\n",
    "        for i in range(taggerChannels):\n",
    "            if (dat['channel']==i):\n",
    "                energies[i].Fill(dat['Energy'])\n",
    "                for time in dat['Time']:\n",
    "                    histos[i].Fill(time)\n",
    "\n",
    "      #  for time in dat['Time']:                     # for each tagger time\n",
    "      #      for i in range(taggerChannels):          # for each channel\n",
    "      #          if (dat['channel']==i):              # if the channel of that data point is the current channel\n",
    "      #              histos[i].Fill(time)             # then put that time in the histogram\n",
    "\n",
    "                    \n",
    "    if(aq.eventNo%5000==0):                          # print a processing statement every 5000 events\n",
    "        print(\"Events Processed: \",aq.eventNo)\n",
    "    \n",
    "aq.runFunction(plotCalTagger,0,500000)               # run this process over 500000 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies[1].GetConten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the data for the tagger channels and apply a gaussian fit (with an offset) to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram 216 is empty (has 0.0 entries).\n",
      "Histogram 273 is empty (has 1.0 entries).\n",
      "Histogram 327 is empty (has 1.0 entries).\n"
     ]
    }
   ],
   "source": [
    "tagger_test = ROOT.TFile(\"tagger_test.root\",\"RECREATE\")     # create a root file to put all the histograms into\n",
    "\n",
    "myFit = ROOT.TF1(\"myFit\",\"gaus(0)+pol0(3)\")                   # make a gaussian fit with an offset/background\n",
    "myFit.SetParNames(\"constant\",\"mean\",\"stdev\",\"background\")     # label the parameters\n",
    "\n",
    "badFitChannels = []      # set up an array to remember the empty or near empty histograms\n",
    "\n",
    "con = [None]*taggerChannels           # set up an array for the constant value\n",
    "peak = [None]*taggerChannels          # set up an array for the peak locations\n",
    "sigma = [None]*taggerChannels         # set up an array for the standard deviations of the peaks\n",
    "bgLevel = [None]*taggerChannels       # set up an array for the background levels\n",
    "\n",
    "sig = 1.1    # initialize a standard deviation value to try\n",
    "bg = 100     # initialize a background value to try\n",
    "\n",
    "for i in range(taggerChannels):              # for each channel in the tagger                                   \n",
    "    histos[i].SetTitle(\"Channel \"+str(i))    # set the title for the histogram              \n",
    "    \n",
    "    if(histos[i].GetEntries()>10):                         # if the histogram has entries\n",
    "        binmax = histos[i].GetMaximumBin()                 # find the maximum number of entries in a bin\n",
    "        pk = histos[i].GetXaxis().GetBinCenter(binmax)     # find the bin corresponding to the max\n",
    "        \n",
    "        myFit.SetParameter(1,pk)          # try a peak value near the fullest bin\n",
    "        myFit.SetParameter(2,sig)         # try this standard deviation value for the fit       \n",
    "        myFit.SetParameter(3,bg)          # try this background value for the fit\n",
    "        \n",
    "        histos[i].Fit(\"myFit\",\"SQ\",\"\",-300,600)    # fit the histogram with a gaussian\n",
    "        \n",
    "        con[i] = histos[i].GetFunction(\"myFit\").GetParameter(\"constant\")           # save the constant value\n",
    "        peak[i] = histos[i].GetFunction(\"myFit\").GetParameter(\"mean\")              # save the peak position\n",
    "        sigma[i] = histos[i].GetFunction(\"myFit\").GetParameter(\"stdev\")            # save the standard deviation\n",
    "        bgLevel[i] = histos[i].GetFunction(\"myFit\").GetParameter(\"background\")     # save the background level\n",
    "        \n",
    "        # check to make sure we're getting the fit we want\n",
    "        if(ROOT.gMinuit.fCstatu!=\"CONVERGED \" or not(120<peak[i]<160) or not(0<sigma[i]<4) or bgLevel[i]<0 or con[i]<0):  \n",
    "            print(\"Channel \"+str(i)+\" fit has failed.\")     # if this is printed, see OldTaggerCalib.ipynb for possible while loop\n",
    "            badFitChannels.append(i)                        # note that this channel was not fit\n",
    "        \n",
    "        sig = sigma[i]       # try using this sigma value for the next channel\n",
    "        bg = bgLevel[i]      # try using this background value for the next channel\n",
    "       \n",
    "    else:       # if the histogram is empty                                     \n",
    "        print(\"Histogram \"+str(i)+\" is empty (has \"+str(histos[i].GetEntries())+\" entries).\")    # then say so \n",
    "        badFitChannels.append(i)               # note that this channel was not fit\n",
    "     \n",
    "    histos[i].Write(\"tagger_test\")     # write the histogram to a root file for viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding how much each peak is off from zero, and setting it back to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagger_calib = ROOT.TFile(\"tagger_calib.root\",\"RECREATE\")    # create a root file to put all the histograms into\n",
    "\n",
    "histos_calib = [None]*taggerChannels           # set up an array of 368 histograms to fill\n",
    "\n",
    "for i in range(taggerChannels):                # for each channel in the tagger\n",
    "    histos_calib[i] = Hist(1300,-500,800)      # create a histogram\n",
    "\n",
    "for i in range(taggerChannels):         # for each channel in the tagger             \n",
    "    if (peak[i]!=None):                 # so long as there is a peak value saved\n",
    "        histos_calib[i] = Hist(1300,int(-500-peak[i]),int(800-peak[i]))     # create a histogram shifted back by the value of the peak      \n",
    "        for j in range(1300):                                               # for each bin in that channel's data\n",
    "            histos_calib[i].SetBinContent(j,histos[i].GetBinContent(j))     # give the bin that channel's data, now shifted back by the peak value\n",
    "     \n",
    "    histos_calib[i].SetTitle(\"Channel \"+str(i))     # set the title for the histogram\n",
    "    \n",
    "    histos_calib[i].Write(\"tagger_calib\")           # write the histogram to a root file for viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must import the json file, change the channel offset values, and then save the array again as a new json file. Note that the empty histograms write a value of 'None' to the json file for their offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibData = json.load(open('taggerNewer.json'), object_pairs_hook=OrderedDict)     # load the tagger calibration data and keep its original ordering\n",
    "\n",
    "i = 0                                   # starting at channel 0\n",
    "for chan in calibData['channels']:      # for each of the tagger channels\n",
    "    chan['offset'][0] = peak[i]         # change the offset value to the one found in our earlier fit for that channel\n",
    "    i+=1                                # move on to the next channel\n",
    "\n",
    "json.dump(calibData, open('taggerWithOffsets.json', 'w'), indent = 2)              # write the changed data to a new json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
