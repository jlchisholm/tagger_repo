{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is currently the best code for calibrating the tagger timing and saving the offsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (Possible) Code for Calibrating the Tagger Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the tools we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.16/00\n"
     ]
    }
   ],
   "source": [
    "import Acqu as aq\n",
    "import AcquDetector as aqdet\n",
    "import ROOT\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from rootpy.plotting import histogram, Hist2D, Hist, Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data to be analyzed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mk2 Data\n"
     ]
    }
   ],
   "source": [
    "inFile = '/scratch/2019-05_Timepix/Timepix_33.dat'\n",
    "aq.openFile(inFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the detector file for the Tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taggerNewer.json\n"
     ]
    }
   ],
   "source": [
    "aqdet.LoadDetectors(['taggerNewer.json'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up histograms for each of the channels, and then fill them with tagger times from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Events Processed: ', 5000)\n",
      "('Events Processed: ', 10000)\n",
      "('Events Processed: ', 15000)\n",
      "('Events Processed: ', 20000)\n",
      "('Events Processed: ', 25000)\n",
      "('Events Processed: ', 30000)\n",
      "('Events Processed: ', 35000)\n",
      "('Events Processed: ', 40000)\n",
      "('Events Processed: ', 45000)\n",
      "('Events Processed: ', 50000)\n",
      "('Events Processed: ', 55000)\n",
      "('Events Processed: ', 60000)\n",
      "('Events Processed: ', 65000)\n",
      "('Events Processed: ', 70000)\n",
      "('Events Processed: ', 75000)\n",
      "('Events Processed: ', 80000)\n",
      "('Events Processed: ', 85000)\n",
      "('Events Processed: ', 90000)\n",
      "('Events Processed: ', 95000)\n",
      "('Events Processed: ', 100000)\n",
      "('Events Processed: ', 105000)\n",
      "('Events Processed: ', 110000)\n",
      "('Events Processed: ', 115000)\n",
      "('Events Processed: ', 120000)\n",
      "('Events Processed: ', 125000)\n",
      "('Events Processed: ', 130000)\n",
      "('Events Processed: ', 135000)\n",
      "('Events Processed: ', 140000)\n",
      "('Events Processed: ', 145000)\n",
      "('Events Processed: ', 150000)\n",
      "('Events Processed: ', 155000)\n",
      "('Events Processed: ', 160000)\n",
      "('Events Processed: ', 165000)\n",
      "('Events Processed: ', 170000)\n",
      "('Events Processed: ', 175000)\n",
      "('Events Processed: ', 180000)\n",
      "('Events Processed: ', 185000)\n",
      "('Events Processed: ', 190000)\n",
      "('Events Processed: ', 195000)\n",
      "('Events Processed: ', 200000)\n",
      "('Events Processed: ', 205000)\n",
      "('Events Processed: ', 210000)\n",
      "('Events Processed: ', 215000)\n",
      "('Events Processed: ', 220000)\n",
      "('Events Processed: ', 225000)\n",
      "('Events Processed: ', 230000)\n",
      "('Events Processed: ', 235000)\n",
      "('Events Processed: ', 240000)\n",
      "('Events Processed: ', 245000)\n",
      "('Events Processed: ', 250000)\n",
      "('Events Processed: ', 255000)\n",
      "('Events Processed: ', 260000)\n",
      "('Events Processed: ', 265000)\n",
      "('Events Processed: ', 270000)\n",
      "('Events Processed: ', 275000)\n",
      "('Events Processed: ', 280000)\n",
      "('Events Processed: ', 285000)\n",
      "('Events Processed: ', 290000)\n",
      "('Events Processed: ', 295000)\n",
      "('Events Processed: ', 300000)\n",
      "('Events Processed: ', 305000)\n",
      "('Events Processed: ', 310000)\n",
      "('Events Processed: ', 315000)\n",
      "('Events Processed: ', 320000)\n",
      "('Events Processed: ', 325000)\n",
      "('Events Processed: ', 330000)\n",
      "('Events Processed: ', 335000)\n",
      "('Events Processed: ', 340000)\n",
      "('Events Processed: ', 345000)\n",
      "('Events Processed: ', 350000)\n",
      "('Events Processed: ', 355000)\n",
      "('Events Processed: ', 360000)\n",
      "('Events Processed: ', 365000)\n",
      "('Events Processed: ', 370000)\n",
      "('Events Processed: ', 375000)\n",
      "('Events Processed: ', 380000)\n",
      "('Events Processed: ', 385000)\n",
      "('Events Processed: ', 390000)\n",
      "('Events Processed: ', 395000)\n",
      "('Events Processed: ', 400000)\n",
      "('Events Processed: ', 405000)\n",
      "('Events Processed: ', 410000)\n",
      "('Events Processed: ', 415000)\n",
      "('Events Processed: ', 420000)\n",
      "('Events Processed: ', 425000)\n",
      "('Events Processed: ', 430000)\n",
      "('Events Processed: ', 435000)\n",
      "('Events Processed: ', 440000)\n",
      "('Events Processed: ', 445000)\n",
      "('Events Processed: ', 450000)\n",
      "('Events Processed: ', 455000)\n",
      "('Events Processed: ', 460000)\n",
      "('Events Processed: ', 465000)\n",
      "('Events Processed: ', 470000)\n",
      "('Events Processed: ', 475000)\n",
      "('Events Processed: ', 480000)\n",
      "('Events Processed: ', 485000)\n",
      "('Events Processed: ', 490000)\n",
      "('Events Processed: ', 495000)\n",
      "('Events Processed: ', 500000)\n",
      "('Events Processed: ', 505000)\n",
      "('Events Processed: ', 510000)\n",
      "('Events Processed: ', 515000)\n",
      "('Events Processed: ', 520000)\n",
      "('Events Processed: ', 525000)\n",
      "('Events Processed: ', 530000)\n",
      "('Events Processed: ', 535000)\n",
      "('Events Processed: ', 540000)\n",
      "('Events Processed: ', 545000)\n",
      "('Events Processed: ', 550000)\n",
      "('Events Processed: ', 555000)\n",
      "('Events Processed: ', 560000)\n",
      "('Events Processed: ', 565000)\n",
      "('Events Processed: ', 570000)\n",
      "('Events Processed: ', 575000)\n",
      "('Events Processed: ', 580000)\n",
      "('Events Processed: ', 585000)\n",
      "('Events Processed: ', 590000)\n",
      "('Events Processed: ', 595000)\n",
      "('Events Processed: ', 600000)\n",
      "('Events Processed: ', 605000)\n",
      "('Events Processed: ', 610000)\n",
      "('Events Processed: ', 615000)\n",
      "('Events Processed: ', 620000)\n",
      "('Events Processed: ', 625000)\n",
      "('Events Processed: ', 630000)\n",
      "('Events Processed: ', 635000)\n",
      "('Events Processed: ', 640000)\n",
      "('Events Processed: ', 645000)\n",
      "('Events Processed: ', 650000)\n",
      "('Events Processed: ', 655000)\n",
      "('Events Processed: ', 660000)\n",
      "('Events Processed: ', 665000)\n",
      "('Events Processed: ', 670000)\n",
      "('Events Processed: ', 675000)\n",
      "('Events Processed: ', 680000)\n",
      "('Events Processed: ', 685000)\n",
      "('Events Processed: ', 690000)\n",
      "('Events Processed: ', 695000)\n",
      "('Events Processed: ', 700000)\n",
      "('Events Processed: ', 705000)\n",
      "('Events Processed: ', 710000)\n",
      "('Events Processed: ', 715000)\n",
      "('Events Processed: ', 720000)\n",
      "('Events Processed: ', 725000)\n",
      "('Events Processed: ', 730000)\n",
      "('Events Processed: ', 735000)\n",
      "('Events Processed: ', 740000)\n",
      "('Events Processed: ', 745000)\n",
      "('Events Processed: ', 750000)\n",
      "('Events Processed: ', 755000)\n",
      "('Events Processed: ', 760000)\n",
      "('Events Processed: ', 765000)\n",
      "('Events Processed: ', 770000)\n",
      "('Events Processed: ', 775000)\n",
      "('Events Processed: ', 780000)\n",
      "('Events Processed: ', 785000)\n",
      "('Events Processed: ', 790000)\n",
      "('Events Processed: ', 795000)\n",
      "('Events Processed: ', 800000)\n",
      "('Events Processed: ', 805000)\n",
      "('Events Processed: ', 810000)\n",
      "('Events Processed: ', 815000)\n",
      "('Events Processed: ', 820000)\n",
      "('Events Processed: ', 825000)\n",
      "('Events Processed: ', 830000)\n",
      "('Events Processed: ', 835000)\n",
      "('Events Processed: ', 840000)\n",
      "('Events Processed: ', 845000)\n",
      "('Events Processed: ', 850000)\n",
      "('Events Processed: ', 855000)\n",
      "('Events Processed: ', 860000)\n",
      "('Events Processed: ', 865000)\n",
      "('Events Processed: ', 870000)\n",
      "('Events Processed: ', 875000)\n",
      "('Events Processed: ', 880000)\n",
      "('Events Processed: ', 885000)\n",
      "('Events Processed: ', 890000)\n",
      "('Events Processed: ', 895000)\n",
      "('Events Processed: ', 900000)\n",
      "('Events Processed: ', 905000)\n",
      "('Events Processed: ', 910000)\n",
      "('Events Processed: ', 915000)\n",
      "('Events Processed: ', 920000)\n",
      "('Events Processed: ', 925000)\n",
      "('Events Processed: ', 930000)\n",
      "('Events Processed: ', 935000)\n",
      "('Events Processed: ', 940000)\n",
      "('Events Processed: ', 945000)\n",
      "('Events Processed: ', 950000)\n",
      "('Events Processed: ', 955000)\n",
      "('Events Processed: ', 960000)\n",
      "('Events Processed: ', 965000)\n",
      "('Events Processed: ', 970000)\n",
      "('Events Processed: ', 975000)\n",
      "('Events Processed: ', 980000)\n",
      "('Events Processed: ', 985000)\n",
      "('Events Processed: ', 990000)\n",
      "('Events Processed: ', 995000)\n",
      "('Events Processed: ', 1000000)\n"
     ]
    }
   ],
   "source": [
    "taggerChannels = aqdet.Channels['Tagger']     # number of channels in the tagger (368)\n",
    "\n",
    "histos = [None]*taggerChannels                # set up an array of 368 histograms to fill      \n",
    "\n",
    "for i in range(taggerChannels):               # for each tagger channel\n",
    "    histos[i] = Hist(1300,-500,800)           # set up a histogram for that tagger channel\n",
    "\n",
    "def plotCalTagger():\n",
    "    data = aqdet.Calibrate(aq.adcArray)              # calibrate the data\n",
    "    taggerTimes = aqdet.Arrays['Tagger']['Time']     # get the tagger times\n",
    "    \n",
    "    for dat in taggerTimes:                          # for each data point\n",
    "        for time in dat['Time']:                     # for each tagger time\n",
    "            for i in range(taggerChannels):          # for each channel\n",
    "                if (dat['channel']==i):              # if the channel of that data point is the current channel\n",
    "                    histos[i].Fill(time)             # then put that time in the histogram\n",
    "                    \n",
    "    if(aq.eventNo%5000==0):                          # print a processing statement every 5000 events\n",
    "        print(\"Events Processed: \",aq.eventNo)\n",
    "    \n",
    "aq.runFunction(plotCalTagger,0,1000000)               # run this process over 500000 events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the data for the tagger channels and apply a gaussian fit (with an offset) to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram 216 is empty (has 2.0 entries).\n",
      "Histogram 273 is empty (has 1.0 entries).\n",
      "Histogram 327 is empty (has 2.0 entries).\n"
     ]
    }
   ],
   "source": [
    "ROOT.enableJSVis()\n",
    "\n",
    "tagger_times = ROOT.TFile(\"tagger_times.root\",\"RECREATE\")     # create a root file to put all the histograms into\n",
    "\n",
    "myFit = ROOT.TF1(\"myFit\",\"gaus(0)+pol0(3)\")                   # make a gaussian fit with an offset/background\n",
    "myFit.SetParNames(\"constant\",\"mean\",\"stdev\",\"background\")     # label the parameters\n",
    "\n",
    "empties = []      # set up an array to remember the empty or near empty histograms\n",
    "\n",
    "con = [None]*taggerChannels           # set up an array for the constant value\n",
    "peak = [None]*taggerChannels          # set up an array for the peak locations\n",
    "sigma = [None]*taggerChannels         # set up an array for the standard deviations of the peaks\n",
    "bgLevel = [None]*taggerChannels       # set up an array for the background levels\n",
    "\n",
    "sig = 1.1    # initialize a standard deviation value to try\n",
    "bg = 100     # initialize a background value to try\n",
    "\n",
    "for i in range(taggerChannels):              # for each channel in the tagger                                   \n",
    "    histos[i].SetTitle(\"Channel \"+str(i))    # set the title for the histogram              \n",
    "    \n",
    "    if(histos[i].GetEntries()>10):                         # if the histogram has entries\n",
    "        binmax = histos[i].GetMaximumBin()                 # find the maximum number of entries in a bin\n",
    "        pk = histos[i].GetXaxis().GetBinCenter(binmax)     # find the bin corresponding to the max\n",
    "        \n",
    "        myFit.SetParameter(1,pk)          # try a peak value near the fullest bin\n",
    "        myFit.SetParameter(2,sig)         # try this standard deviation value for the fit       \n",
    "        myFit.SetParameter(3,bg)          # try this background value for the fit\n",
    "        \n",
    "        histos[i].Fit('myFit','SQ','',-300,600)    # fit the histogram with a gaussian\n",
    "        \n",
    "        peak[i] = histos[i].Fit('myFit','SQ','',-300,600).Get().Parameter(1)           # save the peak position\n",
    "        sigma[i] = abs(histos[i].Fit('myFit','SQ','',-300,600).Get().Parameter(2))     # save the standard deviation\n",
    "        bgLevel[i] = histos[i].Fit('myFit','SQ','',-300,600).Get().Parameter(3)        # save the background level\n",
    "        con[i] = histos[i].Fit('myFit','SQ','',-300,600).Get().Parameter(0) \n",
    " \n",
    "        if(ROOT.gMinuit.fCstatu!=\"CONVERGED \" or not(120<peak[i]<160) or not(-2<sigma[i]<4) or bgLevel[i]<0 or con[i]<0):  \n",
    "            print(\"Channel \"+str(i)+\" fit has failed.\")      # if this is printed, see OldTaggerCalib.ipynb for possible while loop\n",
    "            \n",
    "        \n",
    "        sig = sigma[i]       # try using this sigma value for the next channel\n",
    "        bg = bgLevel[i]      # try using this background value for the next channel\n",
    "       \n",
    "    else:       # if the histogram is empty                                     \n",
    "        print(\"Histogram \"+str(i)+\" is empty (has \"+str(histos[i].GetEntries())+\" entries).\")    # then say so \n",
    "        empties.append(i)               # note that this channel is an empty histogram\n",
    "     \n",
    "    histos[i].Write(\"tagger_times\")     # write the histogram to a root file for viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding how much each peak is off from zero, and setting it back to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagger_calib = ROOT.TFile(\"tagger_calib.root\",\"RECREATE\")    # create a root file to put all the histograms into\n",
    "\n",
    "histos_calib = [None]*taggerChannels           # set up an array of 368 histograms to fill\n",
    "\n",
    "for i in range(taggerChannels):                # for each channel in the tagger\n",
    "    histos_calib[i] = Hist(1300,-500,800)      # create a histogram\n",
    "\n",
    "for i in range(taggerChannels):         # for each channel in the tagger             \n",
    "    if (peak[i]!=None):                 # so long as there is a peak value saved\n",
    "        histos_calib[i] = Hist(1300,int(-500-peak[i]),int(800-peak[i]))     # create a histogram shifted back by the value of the peak      \n",
    "        for j in range(1300):                                               # for each bin in that channel's data\n",
    "            histos_calib[i].SetBinContent(j,histos[i].GetBinContent(j))     # give the bin that channel's data, now shifted back by the peak value\n",
    "     \n",
    "    histos_calib[i].SetTitle(\"Channel \"+str(i))     # set the title for the histogram\n",
    "    \n",
    "    histos_calib[i].Write(\"tagger_calib\")           # write the histogram to a root file for viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must import the json file, change the channel offset values, and then save the array again as a new json file. Note that the empty histograms write a value of 'None' to the json file for their offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibData = json.load(open('taggerNewer.json'), object_pairs_hook=OrderedDict)     # load the tagger calibration data and keep its original ordering\n",
    "\n",
    "i = 0                                   # starting at channel 0\n",
    "for chan in calibData['channels']:      # for each of the tagger channels\n",
    "    if(i not in empties):               # so long as the histogram has data fitted\n",
    "        chan['offset'][0] += peak[i]/chan['scale'][0]      # change the offset value to the correct one\n",
    "    i+=1                                # move on to the next channel\n",
    "\n",
    "json.dump(calibData, open('taggerWithOffsets.json', 'w'), indent = 2)              # write the changed data to a new json file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
